# å²—ä½æè¿°æœ¬åœ°Embeddingå¤„ç†ï¼ˆåˆ†å— + è½ç›˜ memmap æ–¹æ¡ˆ Aï¼‰
# ä½¿ç”¨Sentence Transformersæœ¬åœ°æ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥å¤„ç†

import pandas as pd
import numpy as np
import json
import torch
import os
import time
from tqdm import tqdm
import re
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer

# -------- é…ç½® --------
MODEL_NAME = "autodl-tmp/Qwen/Qwen3-Embedding-4B"   # ä½ çš„æ¨¡å‹
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16                   # A40 æ”¯æŒ BF16ï¼›ä¹Ÿå¯æ”¹ torch.float16
ATTN_IMPL = "sdpa"                       # ä¸è£… flash-attn ä¹Ÿèƒ½åƒåˆ°é«˜æ€§èƒ½å†…æ ¸
BATCH_SIZE = 32
CHUNK_SIZE = 50_000                     # æ¯æ‰¹å†™ç›˜çš„æ ·æœ¬æ•°ï¼›æŒ‰å†…å­˜æƒ…å†µå¯è°ƒå¤§/å°
EMB_MEMMAP_PATH = "autodl-tmp/embeddings.f16.memmap"
META_JSON_PATH  = "autodl-tmp/embeddings.meta.json"
INPUT_PARQUET   = "autodl-fs/list_par2.parquet"
INPUT_COLUMN    = "responsibilities"     # ä½ çš„æ–‡æœ¬åˆ—å
# å¸¦å›å†™çš„æ–°æ–‡ä»¶è·¯å¾„
OUTPUT_PARQUET_MATCHED = "autodl-fs/list_par2_with_matches.parquet"
# ----------------------

# æ–°å¢è·¯å¾„ï¼šæ˜ å°„ä¸å½¢çŠ¶æ•°æ®ï¼ˆå‹ç¼©å­˜å‚¨ï¼‰
MAPPING_NPZ_PATH = "autodl-tmp/emb_mapping.npz"     # å­˜æ”¾ exploded -> unique çš„æ˜ å°„
ROW_COUNTS_NPZ_PATH = "autodl-tmp/emb_row_counts.npz"  # å­˜æ”¾æ¯è¡Œçš„å…ƒç´ æ•°é‡ï¼ˆæ¸…æ´—åï¼‰
CKPT_JSON_PATH = META_JSON_PATH + ".ckpt.json"  # æ–­ç‚¹ç»­è·‘å…ƒæ•°æ®

# æ£€æŸ¥å¿…è¦çš„åº“
# -------- å·¥å…·å‡½æ•°ï¼šæ¸…æ´—ä¸ç­¾å --------
def clean_text(x: Any) -> Optional[str]:
    """å°†ä»»æ„å¯¹è±¡è½¬æ¢ä¸ºå¹²å‡€çš„å­—ç¬¦ä¸²ï¼›è¿”å› None è¡¨ç¤ºåº”ä¸¢å¼ƒã€‚
    è§„åˆ™ï¼šå»é¦–å°¾ç©ºç™½ï¼›è¿‡æ»¤ç©ºä¸²ä¸å¸¸è§ç©ºå€¼æ ‡è®°ï¼›åˆå¹¶å¤šä½™ç©ºç™½ã€‚
    """
    if x is None:
        return None
    s = str(x)
    s = s.strip()
    if not s:
        return None
    lower = s.lower()
    if lower in {"nan", "none", "null", "na", "n/a"}:
        return None
    s = re.sub(r"\s+", " ", s)
    return s if s else None


def normalize_to_list(x: Any) -> List[str]:
    """å°†å•å…ƒæ ¼å€¼å½’ä¸€åŒ–ä¸ºæ¸…æ´—åçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
    - åˆ—è¡¨/å…ƒç»„/ndarrayï¼šé€é¡¹æ¸…æ´—
    - å…¶ä»–æ ‡é‡ï¼šå•å…ƒç´ åˆ—è¡¨
    - ç¼ºå¤±å€¼ï¼šç©ºåˆ—è¡¨
    """
    if isinstance(x, (list, tuple, np.ndarray)):
        items = x
    else:
        # æ³¨æ„ï¼špd.isna å¯¹åˆ—è¡¨ä¼šæŠ¥é”™ï¼Œå› æ­¤ä»…åœ¨éåºåˆ—æ—¶åˆ¤æ–­
        items = [] if (x is None or (not isinstance(x, (list, tuple, np.ndarray)) and pd.isna(x))) else [x]
    cleaned: List[str] = []
    for it in items:
        t = clean_text(it)
        if t is not None:
            cleaned.append(t)
    return cleaned


def compute_input_signature(input_file: str, column_name: str, num_rows: int, total_texts: int, unique_texts: int) -> str:
    """ç”¨äºæ–­ç‚¹ç»­è·‘çš„ä¸€è‡´æ€§æ ¡éªŒï¼Œå°½é‡ä¸å¼•å…¥æ˜‚è´µå“ˆå¸Œã€‚
    ç”±æ–‡ä»¶ç»å¯¹è·¯å¾„ã€mtimeã€size ä¸ç»Ÿè®¡é‡æ„æˆã€‚
    """
    try:
        st = os.stat(input_file)
        return f"{os.path.abspath(input_file)}|{column_name}|{st.st_size}|{int(st.st_mtime)}|rows={num_rows}|M={total_texts}|U={unique_texts}"
    except Exception:
        return f"{os.path.abspath(input_file)}|{column_name}|rows={num_rows}|M={total_texts}|U={unique_texts}"


# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ° embedding æ¨¡å‹...")
try:
    model = SentenceTransformer(
        MODEL_NAME,
        device=DEVICE,
        model_kwargs={
            "attn_implementation": ATTN_IMPL,
            "dtype": DTYPE,       
        },
        tokenizer_kwargs={
            "padding_side": "left"      
        },
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: max_seq_len={model.max_seq_length}, dim={model.get_sentence_embedding_dimension()}")
except Exception as e:
    raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\nğŸ’¡ é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")

# è¯»å– + æ¸…æ´— + å½¢çŠ¶ä¿ç•™
print("ğŸ“¥ è¯»å–å¹¶æ¸…æ´—æ•°æ®...")
df = pd.read_parquet(INPUT_PARQUET)
col = df[INPUT_COLUMN]

# æŒ‰è¡Œæ¸…æ´—ï¼Œä¿ç•™åˆ—è¡¨å½¢çŠ¶
row_lists: List[List[str]] = [normalize_to_list(v) for v in col.tolist()]
row_counts = np.array([len(lst) for lst in row_lists], dtype=np.int32)
N = len(row_lists)
M = int(row_counts.sum())
print(f"âœ… åŸå§‹è¡Œæ•°: {N:,}ï¼Œæ¸…æ´—åæ€»æ–‡æœ¬æ•°: {M:,}")

# å±•å¹³å¹¶å…¨å±€å»é‡ï¼Œæ„å»º exploded -> unique çš„æ˜ å°„
print("ğŸ” å»é‡å¹¶æ„å»ºæ˜ å°„...")
texts: List[str] = [t for lst in row_lists for t in lst]
text_to_uid: Dict[str, int] = {}
unique_texts: List[str] = []
mapping = np.empty(M, dtype=np.uint32 if M < (1 << 32) else np.int64)
write_index = 0
for t in texts:
    uid = text_to_uid.get(t)
    if uid is None:
        uid = len(unique_texts)
        text_to_uid[t] = uid
        unique_texts.append(t)
    mapping[write_index] = uid
    write_index += 1
U = len(unique_texts)
print(f"âœ… å”¯ä¸€æ–‡æœ¬æ•°: {U:,}  ï¼ˆå»é‡ç‡ {(1 - (U / M) if M else 0) * 100:.2f}%ï¼‰")

# å†™å…¥å½¢çŠ¶ä¸æ˜ å°„ï¼ˆå‹ç¼© npzï¼‰
np.savez_compressed(ROW_COUNTS_NPZ_PATH, counts=row_counts)
np.savez_compressed(MAPPING_NPZ_PATH, mapping=mapping)
print(f"ğŸ§¾ å·²å†™å‡ºå½¢çŠ¶ä¸æ˜ å°„: {ROW_COUNTS_NPZ_PATH}, {MAPPING_NPZ_PATH}")

# é¢„åˆ†é…ç£ç›˜æ˜ å°„çŸ©é˜µï¼ˆfloat16 å­˜å‚¨ï¼Œç©ºé—´æ›´çœï¼‰
d = model.get_sentence_embedding_dimension()
if U == 0:
    # æ— å¯ç¼–ç æ–‡æœ¬ï¼Œä»…è¾“å‡ºå…ƒæ•°æ®å¹¶é€€å‡º
    meta = {
        "version": 2,
        "model": MODEL_NAME,
        "count_total": int(M),
        "unique_count": int(U),
        "num_rows": int(N),
        "embedding_dim": int(d),
        "stored_dtype": "float16",
        "normalize_embeddings": True,
        "attn_implementation": ATTN_IMPL,
        "torch_dtype": str(DTYPE).replace("torch.", ""),
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "source": {"file": INPUT_PARQUET, "column": INPUT_COLUMN},
        "row_counts_path": ROW_COUNTS_NPZ_PATH,
        "mapping_path": MAPPING_NPZ_PATH,
    }
    with open(META_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print("âœ… æ— å¯ç¼–ç æ–‡æœ¬ï¼Œå·²ä»…å†™å‡ºå…ƒæ•°æ®ä¸å½¢çŠ¶/æ˜ å°„æ–‡ä»¶ã€‚")
    raise SystemExit(0)

print("ğŸ“ é¢„åˆ†é…/æ‰“å¼€ memmap æ–‡ä»¶...")
input_sig = compute_input_signature(INPUT_PARQUET, INPUT_COLUMN, N, M, U)

write_ptr = 0
mode = "w+"
if os.path.exists(EMB_MEMMAP_PATH) and os.path.exists(CKPT_JSON_PATH):
    try:
        with open(CKPT_JSON_PATH, "r", encoding="utf-8") as f:
            ckpt = json.load(f)
        if (
            ckpt.get("input_signature") == input_sig
            and ckpt.get("model") == MODEL_NAME
            and int(ckpt.get("embedding_dim", -1)) == int(d)
            and int(ckpt.get("unique_count", -1)) == int(U)
        ):
            write_ptr = int(ckpt.get("write_ptr", 0))
            mode = "r+"
            print(f"ğŸ” æ£€æµ‹åˆ°æ–­ç‚¹ç»­è·‘ï¼Œå·²å®Œæˆ {write_ptr:,}/{U:,}")
        else:
            print("âš ï¸ æ–­ç‚¹æ–‡ä»¶ä¸åŒ¹é…ï¼Œå°†é‡æ–°å¼€å§‹è®¡ç®—ã€‚")
    except Exception:
        pass

emb_mm = np.memmap(EMB_MEMMAP_PATH, dtype="float16", mode=mode, shape=(U, d))

# åˆ†å—ç¼–ç å¹¶å†™å…¥ç£ç›˜
print("ğŸš€ å¼€å§‹åˆ†å—ç¼–ç å¹¶è½ç›˜ï¼ˆå»é‡åå†™å…¥ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰...")
for start in tqdm(range(write_ptr, U, CHUNK_SIZE), desc="Encoding (chunked)"):
    end = min(start + CHUNK_SIZE, U)
    batch_texts = unique_texts[start:end]

    vec = model.encode(
        batch_texts,
        batch_size=BATCH_SIZE,
        normalize_embeddings=True,     # å•ä½åŒ–ï¼Œä¾¿äºåç»­ç”¨ç‚¹ä¹˜/å†…ç§¯
        convert_to_numpy=True,         # ç›´æ¥å›åˆ° CPU/numpyï¼Œçœ VRAM
        show_progress_bar=False,
    )

    # é™ä¸º float16 å†å†™ç›˜ï¼ˆå•ä½åŒ–åç²¾åº¦å½±å“å¾ˆå°ï¼‰
    emb_mm[start:end] = vec.astype("float16", copy=False)

    # åŠæ—¶åˆ·ç›˜ + é‡Šæ”¾æ˜¾å­˜
    emb_mm.flush()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # æ›´æ–°æ–­ç‚¹ä¿¡æ¯
    ckpt = {
        "model": MODEL_NAME,
        "embedding_dim": int(d),
        "unique_count": int(U),
        "write_ptr": int(end),
        "input_signature": input_sig,
        "updated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "row_counts_path": ROW_COUNTS_NPZ_PATH,
        "mapping_path": MAPPING_NPZ_PATH,
        "memmap_path": EMB_MEMMAP_PATH,
        "finished": False,
    }
    with open(CKPT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(ckpt, f, ensure_ascii=False, indent=2)

# å…³é—­ memmapï¼ˆåˆ é™¤å¯¹è±¡è§¦å‘åˆ·ç›˜ï¼‰
del emb_mm

# å†™å…¥å…ƒæ•°æ®ï¼Œä¾¿äºåç»­è¯»å–
meta = {
    "version": 2,
    "model": MODEL_NAME,
    "count_total": int(M),
    "unique_count": int(U),
    "num_rows": int(N),
    "embedding_dim": int(d),
    "stored_dtype": "float16",
    "normalize_embeddings": True,
    "attn_implementation": ATTN_IMPL,
    "torch_dtype": str(DTYPE).replace("torch.", ""),
    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    "source": {"file": INPUT_PARQUET, "column": INPUT_COLUMN},
    "row_counts_path": ROW_COUNTS_NPZ_PATH,
    "mapping_path": MAPPING_NPZ_PATH,
}
with open(META_JSON_PATH, "w", encoding="utf-8") as f:
    json.dump(meta, f, ensure_ascii=False, indent=2)

# æœ€ç»ˆæ›´æ–°æ–­ç‚¹ä¿¡æ¯
final_ckpt = {
    "model": MODEL_NAME,
    "embedding_dim": int(d),
    "unique_count": int(U),
    "write_ptr": int(U),
    "input_signature": input_sig,
    "updated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    "row_counts_path": ROW_COUNTS_NPZ_PATH,
    "mapping_path": MAPPING_NPZ_PATH,
    "memmap_path": EMB_MEMMAP_PATH,
    "finished": True,
}
with open(CKPT_JSON_PATH, "w", encoding="utf-8") as f:
    json.dump(final_ckpt, f, ensure_ascii=False, indent=2)

print("âœ… å…¨éƒ¨å®Œæˆ")
print(f"ğŸ“¦ å‘é‡å·²å†™å…¥: {EMB_MEMMAP_PATH}  ï¼ˆçº¦ {U*d*2/1e9:.2f} GBï¼‰")
print(f"ğŸ§¾ å…ƒæ•°æ®: {META_JSON_PATH}")
print(f"ğŸ”— å½¢çŠ¶ä¸æ˜ å°„: {ROW_COUNTS_NPZ_PATH}, {MAPPING_NPZ_PATH}")

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä»¥åè¯»å–æ—¶ï¼‰ï¼š
# emb_unique = np.memmap(EMB_MEMMAP_PATH, dtype="float16", mode="r", shape=(U, d))
# counts = np.load(ROW_COUNTS_NPZ_PATH)["counts"]
# mapping = np.load(MAPPING_NPZ_PATH)["mapping"]
# emb_exploded = emb_unique[mapping]  # å½¢çŠ¶ï¼š(M, d)
# # æ¢å¤ä¸ºåˆ—è¡¨å½¢çŠ¶ï¼š
# offsets = np.cumsum(counts)[:-1]
# emb_per_row = np.split(emb_exploded, offsets)
# ç›´æ¥æŠŠ emb_unique äº¤ç»™ Faiss/HNSWlib/Annoy å»ºç´¢å¼•å³å¯ï¼›æˆ–åˆ†å—è¯»å–ä»¥èŠ‚çœå†…å­˜ã€‚

############################################
# ç¬¬äºŒå‘é‡åº“ï¼šEmbedding + FAISS æœ€è¿‘é‚»æ£€ç´¢  #
############################################

# -------- é…ç½®ï¼ˆè¯·æŒ‰éœ€ä¿®æ”¹ï¼‰ --------
SECOND_PARQUET = "autodl-fs/second_lib.parquet"   # ç¬¬äºŒåº“æ•°æ®æ–‡ä»¶ï¼ˆçº¦ 2k æ¡ï¼‰
SECOND_COLUMN  = "text"                             # ç¬¬äºŒåº“æ–‡æœ¬åˆ—å

# ç¬¬äºŒåº“å‘é‡è½ç›˜ï¼ˆä¸ä¸Šæ–¹é£æ ¼ä¿æŒä¸€è‡´ï¼‰
SECOND_EMB_MEMMAP_PATH = "autodl-tmp/second_lib.emb.f16.memmap"
SECOND_META_JSON_PATH  = "autodl-tmp/second_lib.emb.meta.json"

# æœ€è¿‘é‚»æœç´¢è¾“å‡ºï¼ˆé’ˆå¯¹æœ¬è„šæœ¬åˆšå†™å‡ºçš„ emb_unique ä¸ç¬¬äºŒåº“ï¼‰
NN_TOPK = 1  # éœ€è¦çš„ TopKï¼ˆé¢˜æ„ä¸ºâ€œæœ€ç›¸ä¼¼å…ƒç´ â€ï¼Œé»˜è®¤ 1ï¼Œå¯æ”¹ï¼‰
NN_INDICES_MEMMAP_PATH = "autodl-tmp/nn_topk.indices.i32.memmap"
NN_SCORES_MEMMAP_PATH  = "autodl-tmp/nn_topk.scores.f32.memmap"
NN_META_JSON_PATH      = "autodl-tmp/nn_search.meta.json"
Q_CHUNK_SIZE = 200_000  # æŸ¥è¯¢åˆ†å—å¤§å°ï¼ˆæ ¹æ®å†…å­˜å¯è°ƒæ•´ï¼‰
# GPU FAISS ç›¸å…³
USE_FAISS_GPU = True
GPU_DEVICE_ID = 0
FAISS_GPU_USE_FP16 = True  # å°†ç´¢å¼•å‘é‡ä»¥ FP16 å­˜å‚¨ï¼Œé™ä½ VRAM å ç”¨
# ----------------------------------

print("ğŸ” å‡†å¤‡å¯¹ç¬¬äºŒåº“è¿›è¡Œ embeddingï¼Œå¹¶ç”¨ FAISS æ£€ç´¢...")

# ç‹¬ç«‹é‡è·‘è‡ªæ£€ï¼šå¦‚éœ€è¦ï¼Œè¡¥é½å…³é”®å¸¸é‡ä¸æ¨¡å‹
if "META_JSON_PATH" not in globals():
    META_JSON_PATH = "autodl-tmp/embeddings.meta.json"
if "EMB_MEMMAP_PATH" not in globals():
    EMB_MEMMAP_PATH = "autodl-tmp/embeddings.f16.memmap"
if "MAPPING_NPZ_PATH" not in globals():
    MAPPING_NPZ_PATH = "autodl-tmp/emb_mapping.npz"
if "ROW_COUNTS_NPZ_PATH" not in globals():
    ROW_COUNTS_NPZ_PATH = "autodl-tmp/emb_row_counts.npz"
if "INPUT_PARQUET" not in globals():
    INPUT_PARQUET = "autodl-fs/list_par2.parquet"
if "INPUT_COLUMN" not in globals():
    INPUT_COLUMN = "responsibilities"
if "OUTPUT_PARQUET_MATCHED" not in globals():
    OUTPUT_PARQUET_MATCHED = "autodl-fs/list_par2_with_matches.parquet"

# åŸºç¡€åº“å¯¼å…¥ï¼ˆç‹¬ç«‹é‡è·‘æ—¶è¡¥é½ï¼‰
try:
    json  # type: ignore[name-defined]
except NameError:
    import json
try:
    np  # type: ignore[name-defined]
except NameError:
    import numpy as np
try:
    pd  # type: ignore[name-defined]
except NameError:
    import pandas as pd
try:
    os  # type: ignore[name-defined]
except NameError:
    import os
try:
    time  # type: ignore[name-defined]
except NameError:
    import time
try:
    List  # type: ignore[name-defined]
except NameError:
    from typing import List

# è‹¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œåˆ™æŒ‰é»˜è®¤é…ç½®åŠ è½½
try:
    model  # type: ignore[name-defined]
except NameError:
    try:
        from sentence_transformers import SentenceTransformer  # noqa: F401
        import torch  # noqa: F401
    except Exception:
        pass
    if "MODEL_NAME" not in globals():
        MODEL_NAME = "autodl-tmp/Qwen/Qwen3-Embedding-4B"
    if "DEVICE" not in globals():
        DEVICE = "cuda" if ("torch" in globals() and getattr(torch, "cuda", None) and torch.cuda.is_available()) else "cpu"
    if "DTYPE" not in globals():
        DTYPE = torch.bfloat16 if "torch" in globals() else None
    if "ATTN_IMPL" not in globals():
        ATTN_IMPL = "sdpa"
    if "BATCH_SIZE" not in globals():
        BATCH_SIZE = 32
    print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ° embedding æ¨¡å‹(ç‹¬ç«‹é‡è·‘)...")
    model = SentenceTransformer(
        MODEL_NAME,
        device=DEVICE,
        model_kwargs={
            "attn_implementation": ATTN_IMPL,
            "dtype": DTYPE,
        },
        tokenizer_kwargs={
            "padding_side": "left",
        },
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ(ç‹¬ç«‹é‡è·‘)")

# æ”¯æŒç‹¬ç«‹é‡è·‘ï¼šåŠ è½½ç¬¬ä¸€æ­¥äº§ç‰©çš„å…ƒæ•°æ®ã€å½¢çŠ¶ä¸æ˜ å°„
try:
    with open(META_JSON_PATH, "r", encoding="utf-8") as f:
        _meta0 = json.load(f)
    d_meta = int(_meta0.get("embedding_dim", 0))
    U_meta = int(_meta0.get("unique_count", 0))
    row_counts_path = _meta0.get("row_counts_path", ROW_COUNTS_NPZ_PATH)
    mapping_path = _meta0.get("mapping_path", MAPPING_NPZ_PATH)
    # æ ¡éªŒç»´åº¦ä¸å½“å‰æ¨¡å‹ä¸€è‡´
    d_model = model.get_sentence_embedding_dimension()
    if d_meta != d_model:
        raise RuntimeError(
            f"âŒ ç¬¬ä¸€åº“å‘é‡ç»´åº¦({d_meta})ä¸å½“å‰æ¨¡å‹ç»´åº¦({d_model})ä¸ä¸€è‡´ï¼Œè¯·ç¡®è®¤ä½¿ç”¨åŒä¸€æ¨¡å‹ã€‚"
        )
    d = d_model
    U = U_meta
    row_counts = np.load(row_counts_path)["counts"]
    mapping = np.load(mapping_path)["mapping"]
except Exception as _err:
    raise RuntimeError("âŒ é‡è·‘ç¬¬äºŒæ­¥å¤±è´¥ï¼šæ— æ³•åŠ è½½ç¬¬ä¸€æ­¥çš„å…ƒæ•°æ®/å½¢çŠ¶/æ˜ å°„ã€‚") from _err

# 1) åŠ è½½å·²å­˜åœ¨çš„ç¬¬äºŒåº“å‘é‡ï¼ˆmemmap æˆ–é€šè¿‡ meta æ¨æ–­ï¼‰
print("ğŸ“¥ åŠ è½½ç¬¬äºŒåº“å‘é‡...")
_second_emb_path = globals().get("SECOND_EMB_MEMMAP_PATH", None)
if not _second_emb_path or not os.path.exists(_second_emb_path):
    # å°è¯•é€šè¿‡ meta è¯»å–
    try:
        with open(SECOND_META_JSON_PATH, "r", encoding="utf-8") as f:
            _meta2 = json.load(f)
        _second_emb_path = _meta2.get("memmap_path")
        d_from_meta = int(_meta2.get("embedding_dim", 0))
        if d_from_meta:
            d = d_from_meta
    except Exception:
        pass
if not _second_emb_path or not os.path.exists(_second_emb_path):
    raise FileNotFoundError("âŒ æœªæ‰¾åˆ°ç¬¬äºŒåº“å‘é‡æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ SECOND_EMB_MEMMAP_PATH æˆ– SECOND_META_JSON_PATH")

# ä»å…ƒæ•°æ®æˆ–æ–‡ä»¶å¤§å°æ¨æ–­ S
try:
    if "_meta2" not in locals():
        with open(SECOND_META_JSON_PATH, "r", encoding="utf-8") as f:
            _meta2 = json.load(f)
    S = int(_meta2.get("count"))
    d_meta2 = int(_meta2.get("embedding_dim", d))
    d = d_meta2 or d
except Exception:
    # å…œåº•ï¼šæ ¹æ®æ–‡ä»¶å¤§å°æ¨æ–­
    st = os.stat(_second_emb_path)
    elem_bytes = 2  # float16
    d = int(d)
    S = st.st_size // (elem_bytes * max(d, 1))

vec2_mm = np.memmap(_second_emb_path, dtype="float16", mode="r", shape=(S, d))
vec2 = np.asarray(vec2_mm, dtype="float32")  # faiss è¦ float32

# 3) æ„å»º FAISS ç´¢å¼•ï¼ˆå†…ç§¯ï¼‰
print("ğŸ—ï¸  æ„å»º FAISS IndexFlatIP ç´¢å¼•ï¼ˆGPU ä¼˜å…ˆï¼‰...")
try:
    import faiss  # pip install faiss-gpu
except Exception as e:
    raise RuntimeError("âŒ å¯¼å…¥ faiss å¤±è´¥ã€‚è¯·å…ˆå®‰è£…ï¼špip install faiss-gpu") from e

if USE_FAISS_GPU:
    if not hasattr(faiss, "StandardGpuResources"):
        raise RuntimeError("âŒ å½“å‰ faiss æœªå¯ç”¨ GPU æ¨¡å—ï¼Œè¯·å®‰è£… GPU ç‰ˆï¼špip install faiss-gpu")
    gpu_id = int(GPU_DEVICE_ID)
    if hasattr(faiss, "get_num_gpus"):
        num_gpus = faiss.get_num_gpus()
        if gpu_id < 0 or gpu_id >= num_gpus:
            raise RuntimeError(f"âŒ å¯ç”¨ GPU æ•°é‡ä¸º {num_gpus}ï¼Œé…ç½®çš„ GPU_DEVICE_ID={gpu_id} ä¸å¯ç”¨")
    res = faiss.StandardGpuResources()
    index_cpu = faiss.IndexFlatIP(d)
    # é‡‡ç”¨ FP16 å­˜å‚¨ä»¥é™ä½ VRAMï¼›å¦‚éœ€ç¦ç”¨ï¼Œè®¾ FAISS_GPU_USE_FP16=False
    co = faiss.GpuClonerOptions()
    co.useFloat16 = bool(FAISS_GPU_USE_FP16)
    index = faiss.index_cpu_to_gpu(res, gpu_id, index_cpu, co)
else:
    index = faiss.IndexFlatIP(d)

index.add(vec2.astype("float32", copy=False))

# 4) åŠ è½½æœ¬è„šæœ¬å†™å‡ºçš„ unique å‘é‡ä½œä¸ºæŸ¥è¯¢ï¼Œè¿›è¡Œåˆ†å—æ£€ç´¢
print("ğŸ” å¼€å§‹å¯¹å·²ä¿å­˜çš„å‘é‡è¿›è¡Œæœ€è¿‘é‚»æ£€ç´¢ï¼ˆæŒ‰ç¬¬äºŒåº“ï¼‰...")
emb_unique_q = np.memmap(EMB_MEMMAP_PATH, dtype="float16", mode="r", shape=(U, d))

# ç»“æœ memmapï¼šç´¢å¼•ä¸ç›¸ä¼¼åº¦
I_mm = np.memmap(NN_INDICES_MEMMAP_PATH, dtype="int32", mode="w+", shape=(U, NN_TOPK))
D_mm = np.memmap(NN_SCORES_MEMMAP_PATH, dtype="float32", mode="w+", shape=(U, NN_TOPK))

# è¿›åº¦æ¡æŒ‰æŸ¥è¯¢å‘é‡æ•°é‡æ¨è¿›
with tqdm(total=U, desc="Searching (vectors)", unit="vec") as pbar:
    for start in range(0, U, Q_CHUNK_SIZE):
        end = min(start + Q_CHUNK_SIZE, U)
        q = emb_unique_q[start:end].astype("float32", copy=False)
        D, I = index.search(q, NN_TOPK)
        I_mm[start:end] = I
        D_mm[start:end] = D
        pbar.update(end - start)

I_mm.flush()
D_mm.flush()
del I_mm, D_mm, emb_unique_q

# 5) æ£€ç´¢å…ƒæ•°æ®
nn_meta = {
    "topk": int(NN_TOPK),
    "query_count": int(U),
    "index_count": int(S),
    "similarity": "cosine via inner product (normalized)",
    "query_memmap": EMB_MEMMAP_PATH,
    "index_memmap": SECOND_EMB_MEMMAP_PATH,
    "index_source": {"file": SECOND_PARQUET, "column": SECOND_COLUMN},
    "model": MODEL_NAME,
    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    "indices_memmap": NN_INDICES_MEMMAP_PATH,
    "scores_memmap": NN_SCORES_MEMMAP_PATH,
}
with open(NN_META_JSON_PATH, "w", encoding="utf-8") as f:
    json.dump(nn_meta, f, ensure_ascii=False, indent=2)

print("âœ… æœ€è¿‘é‚»æ£€ç´¢å®Œæˆ")
print(f"ğŸ“¦ ç¬¬äºŒåº“å‘é‡: {SECOND_EMB_MEMMAP_PATH}  ï¼ˆçº¦ {S*d*2/1e6:.2f} MBï¼‰")
print(f"ğŸ“¦ Top{NN_TOPK} ç´¢å¼•: {NN_INDICES_MEMMAP_PATH}")
print(f"ğŸ“¦ Top{NN_TOPK} åˆ†æ•°: {NN_SCORES_MEMMAP_PATH}")
print(f"ğŸ§¾ æ£€ç´¢å…ƒæ•°æ®: {NN_META_JSON_PATH}")

# å°ç¤ºä¾‹ï¼šè¯»å– Top1 ç»“æœå¹¶æŸ¥çœ‹å¯¹åº”æ–‡æœ¬ï¼ˆå¦‚éœ€è¦ï¼Œå¯å–æ¶ˆæ³¨é‡Šï¼‰
# I = np.memmap(NN_INDICES_MEMMAP_PATH, dtype="int32", mode="r", shape=(U, NN_TOPK))
# D = np.memmap(NN_SCORES_MEMMAP_PATH, dtype="float32", mode="r", shape=(U, NN_TOPK))
# with open(SECOND_META_JSON_PATH, "r", encoding="utf-8") as f:
#     _meta2 = json.load(f)
# top1_indices = I[:10, 0]
# for qi, idx in enumerate(top1_indices):
#     print(qi, idx, second_texts[idx] if 0 <= idx < len(second_texts) else None)

############################################
# å›å†™åŒ¹é…æ–‡æœ¬åˆ°åŸå§‹ DataFrame å¹¶è½ç›˜            #
############################################

print("ğŸ“ æ­£åœ¨å°† Top1 åŒ¹é…æ–‡æœ¬å›å†™åˆ° DataFrame å¹¶ä¿å­˜...")

# ç‹¬ç«‹é‡è·‘æ‰€éœ€ï¼šè¡¥é½ä¾èµ–ã€è·¯å¾„ä¸ç»Ÿè®¡
if "NN_META_JSON_PATH" not in globals():
    NN_META_JSON_PATH = "autodl-tmp/nn_search.meta.json"
if "SECOND_META_JSON_PATH" not in globals():
    SECOND_META_JSON_PATH = "autodl-tmp/second_lib.emb.meta.json"
if "INPUT_PARQUET" not in globals():
    INPUT_PARQUET = "autodl-fs/list_par2.parquet"
if "INPUT_COLUMN" not in globals():
    INPUT_COLUMN = "responsibilities"
if "OUTPUT_PARQUET_MATCHED" not in globals():
    OUTPUT_PARQUET_MATCHED = "autodl-fs/list_par2_with_matches.parquet"

try:
    json  # type: ignore[name-defined]
except NameError:
    import json
try:
    np  # type: ignore[name-defined]
except NameError:
    import numpy as np
try:
    pd  # type: ignore[name-defined]
except NameError:
    import pandas as pd
try:
    os  # type: ignore[name-defined]
except NameError:
    import os
try:
    time  # type: ignore[name-defined]
except NameError:
    import time
try:
    re  # type: ignore[name-defined]
except NameError:
    import re
try:
    List  # type: ignore[name-defined]
except NameError:
    from typing import List

# è‹¥å¿…è¦çš„æ¸…æ´—å‡½æ•°åœ¨å½“å‰ä¼šè¯æœªå®šä¹‰ï¼Œåˆ™è¡¥å……ç²¾ç®€ç‰ˆ
try:
    normalize_to_list  # type: ignore[name-defined]
except NameError:
    def clean_text(x):
        if x is None:
            return None
        s = str(x).strip()
        if not s:
            return None
        if s.lower() in {"nan", "none", "null", "na", "n/a"}:
            return None
        s = re.sub(r"\s+", " ", s)
        return s if s else None
    def normalize_to_list(x):
        if isinstance(x, (list, tuple, np.ndarray)):
            items = x
        else:
            items = [] if (x is None or (not isinstance(x, (list, tuple, np.ndarray)) and pd.isna(x))) else [x]
        out = []
        for it in items:
            t = clean_text(it)
            if t is not None:
                out.append(t)
        return out

# è½½å…¥ç¬¬ä¸€æ­¥ metaã€å½¢çŠ¶ä¸æ˜ å°„ï¼ˆè‹¥å½“å‰ä¼šè¯å°šæœªæœ‰è¿™äº›å˜é‡ï¼‰
try:
    U  # type: ignore[name-defined]
    mapping  # type: ignore[name-defined]
    row_counts  # type: ignore[name-defined]
except NameError:
    if "META_JSON_PATH" not in globals():
        META_JSON_PATH = "autodl-tmp/embeddings.meta.json"
    with open(META_JSON_PATH, "r", encoding="utf-8") as f:
        _meta0 = json.load(f)
    U = int(_meta0.get("unique_count", 0))
    _row_counts_path = _meta0.get("row_counts_path", "autodl-tmp/emb_row_counts.npz")
    _mapping_path = _meta0.get("mapping_path", "autodl-tmp/emb_mapping.npz")
    row_counts = np.load(_row_counts_path)["counts"]
    mapping = np.load(_mapping_path)["mapping"]

# è½½å…¥æ£€ç´¢ metaï¼Œè·å– indices è·¯å¾„ä¸ topk
try:
    with open(NN_META_JSON_PATH, "r", encoding="utf-8") as f:
        _nnm = json.load(f)
    _indices_path = _nnm.get("indices_memmap", "autodl-tmp/nn_topk.indices.i32.memmap")
    _topk_read = int(_nnm.get("topk", 1))
    _second_src = _nnm.get("index_source", {})
except Exception:
    _indices_path = "autodl-tmp/nn_topk.indices.i32.memmap"
    _topk_read = 1
    _second_src = {}

# æ„é€ ç¬¬äºŒåº“æ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äºå°†ç´¢å¼•è¿˜åŸä¸ºæ–‡æœ¬ï¼‰
try:
    _second_file = _second_src.get("file") if isinstance(_second_src, dict) else None
    _second_col = _second_src.get("column") if isinstance(_second_src, dict) else None
except Exception:
    _second_file, _second_col = None, None

if not _second_file:
    _second_file = globals().get("SECOND_PARQUET", "autodl-fs/second_lib.parquet")
if not _second_col:
    _second_col = globals().get("SECOND_COLUMN", "text")

def _safe_read_parquet_second(path: str) -> pd.DataFrame:
    try:
        return pd.read_parquet(path, use_legacy_dataset=True)
    except Exception:
        pass
    try:
        return pd.read_parquet(path, engine="fastparquet")
    except Exception:
        pass
    try:
        import pyarrow.parquet as pq  # type: ignore
        _table = pq.read_table(path)
        return _table.to_pandas()
    except Exception as e:
        raise e

df2_back = _safe_read_parquet_second(_second_file)
if _second_col not in df2_back.columns:
    raise KeyError(f"âŒ ç¬¬äºŒåº“æ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ—: {_second_col}")
second_texts: List[str] = []
for v in df2_back[_second_col].tolist():
    second_texts.extend(normalize_to_list(v))
S = len(second_texts)

# è¯»å– Top1 ç´¢å¼•ï¼ˆU,)
I_ro = np.memmap(_indices_path, dtype="int32", mode="r", shape=(U, _topk_read))
uid_to_top1 = np.asarray(I_ro[:, 0], dtype=np.int64)
del I_ro

# å°† unique çº§åˆ«çš„åŒ¹é…ç»“æœæ˜ å°„å› explodedï¼ˆé•¿åº¦ Mï¼‰
exploded_top1 = uid_to_top1[mapping]

# é€è¡Œç»„è£…æˆåˆ—è¡¨ï¼Œé•¿åº¦ä¸æ¸…æ´—å responsibilities å¯¹é½
matches_per_row: List[List[str]] = []
cursor = 0
for cnt in row_counts.tolist():
    if cnt == 0:
        matches_per_row.append([])
        continue
    idxs = exploded_top1[cursor:cursor + cnt]
    row_matches = [second_texts[int(j)] if 0 <= int(j) < S else None for j in idxs]
    matches_per_row.append(row_matches)
    cursor += cnt

# æ·»åŠ æ–°åˆ—å¹¶å†™å‡º Parquetï¼ˆæ”¯æŒç‹¬ç«‹é‡è·‘æ—¶é‡æ–°è¯»å–åŸæ–‡ä»¶ï¼‰

# ç¡®å®šåŸå§‹æ•°æ®æºè·¯å¾„ï¼šä¼˜å…ˆç”¨ç¬¬ä¸€æ­¥å…ƒæ•°æ®ä¸­çš„ source.file
try:
    with open(META_JSON_PATH, "r", encoding="utf-8") as _fsrc:
        _meta_for_source = json.load(_fsrc)
    _src_file = _meta_for_source.get("source", {}).get("file", INPUT_PARQUET)
except Exception:
    _src_file = INPUT_PARQUET

def _safe_read_parquet(path: str) -> pd.DataFrame:
    # 1) pandas + pyarrow legacy è·¯å¾„
    try:
        return pd.read_parquet(path, use_legacy_dataset=True)
    except Exception:
        pass
    # 2) pandas + fastparquetï¼ˆå¯é€‰å®‰è£…ï¼‰
    try:
        return pd.read_parquet(path, engine="fastparquet")
    except Exception:
        pass
    # 3) ç›´æ¥ç”¨ pyarrow è¯»å–
    try:
        import pyarrow.parquet as pq  # type: ignore
        _table = pq.read_table(path)
        return _table.to_pandas()
    except Exception as e:
        raise e

# è‹¥ df å·²åœ¨å†…å­˜ä¸”å½¢çŠ¶åŒ¹é…å°±ç”¨ä¹‹ï¼Œå¦åˆ™ç¨³å¦¥åœ°é‡è¯»
try:
    _df_tmp = df  # type: ignore[name-defined]
    if INPUT_COLUMN not in _df_tmp.columns or len(_df_tmp) != len(matches_per_row):
        raise RuntimeError("reload")
except Exception:
    _df_tmp = _safe_read_parquet(_src_file)

_df_tmp[INPUT_COLUMN + "_match"] = matches_per_row
_df_tmp.to_parquet(OUTPUT_PARQUET_MATCHED, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜å›å†™ç»“æœ: {OUTPUT_PARQUET_MATCHED}")